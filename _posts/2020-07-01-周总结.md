## Transfusion: Understanding Transfer Learning for Medical Imaging

这篇论文是由Google与Bengio等人合著，并发表在了[NIPS2019](https://arxiv.org/pdf/1902.07208.pdf)上面，题目译文：迁移融合：理解医学成像中的迁移学习。

本文的**主要目的**是探讨迁移学习在医学图像处理中的作用，即讨论迁移学习对于医学任务来说是不是必要的。当然，医学成像有多种数据库，本文主要使用彩色眼底图像（DR检测）和X-ray胸片（CheXpert）上进行实验。

论文的**主要发现**有以下几点：

- 迁移学习不会显著影响医疗成像任务的性能，从零开始训练(train from scratch)的模型的性能几乎与标准 ImageNet迁移模型接近；
- 转移学习提供的性能增益有限，甚至使用更小的架构CBR也可以和标准的`ImageNet`模型性能相当（以往学界认为大型网络可以显著提升性能），为 ImageNet 设计的大型模型对于非常小的数据集可能过于高估；
- 由于 CBR 模型比标准 ImageNet 模型小得多、更浅，因此在 ImageNet 分类中性能差得多，这突出表明 ImageNet 性能并不代表医疗任务的性能。标准ImageNet的性能差异和模型的过度参数化(`over-parametrization`)有关，并非源于精细的特征重用；
- 有意义的特性重用只集中在最低层(前两层)，而且大型预训练模型在细调(`fine-tune`)的时候，模型参数变化缓慢（即使在random initialization设置中），重用最低层会使收敛速度得到最大的提高，如图1所示。

<img src="https://raw.githubusercontent.com/fdujay/online_img/master/img/image-20200702233153916.png" alt="image-20200702233153916" style="zoom: 50%;" />

> 图1 重用权重子集与收敛速度的关系。

在对论文通读后，我认为对于医学图像处理，本论文的**主要贡献**如下：

- 印证了在MIA中，我们同样高估了迁移学习的能力，真正有用的信息只集中在前两层；
- 说明了迁移学习预训练中学习到的特征并没有想象中的强泛化(generalization)能力([Quoc V Le et al.](https://arxiv.org/abs/1805.08974v2))。

**启发和思考：**

对于医学图像处理这种**细粒度**分类问题，ImageNet迁移学习不一定能够帮助我们学习良好的特征提取；**即使是非常相似的领域，迁移学习也不一定能提升性能**([Kaiming He et al.](https://arxiv.org/abs/1811.08883))，在医学成像中，由于成像设备和扫描参数等的差异，医学图像的质量会存在较大差异，从而导致模型在临床应用中的性能下降。如何有效解决域迁移性能下降的问题，是困扰学术界的核心问题之一。

> 另外，对于我关注的自监督领域，虽然同样也是得到预训练模型，但是与迁移学习不同的是，自监督预训练模型使用目标域的无标注数据训练，因此可以更好的提取当前任务的特征，不用考虑特征重用的问题，从根本上杜绝了`domain gap`的问题。**也许在临床应用中，利用好大量无标注的数据比构建所谓的Medical ImageNet更重要。**

**补充：**

个人以为医学分类任务非常接近细粒度(Fine-grained)分类问题：即识别细分类别的任务，一般它需要同时使用全局图像信息与局部图像信息精准识别图像子类别。

医学成像任务往往从感兴趣的身体区域的大图像开始，并使用局部纹理的变化来识别病理。如图2所示，在视网膜眼底图像中，红色小点是微动脉瘤和糖尿病视网膜病变的标志，在胸部x光片中，局部白色不透明斑块是结节和肺炎的标志。这与像ImageNet这样的自然图像数据集形成了对比，在自然图像数据集中，图像通常有一个清晰的全局主题。

![image-20200701180647762](https://raw.githubusercontent.com/fdujay/online_img/master/img/image-20200701180647762.png)

> 图2:分别来自ImageNet、视网膜眼底照片和CheXpert数据集的示例图像。眼底照片和胸部x光片比ImageNet图像分辨率高得多，通过寻找组织的局部小变化来分类。

## How Useful is Self-Supervised Pretraining for Visual Tasks?

这篇文章发表在今年的[CVPR2020](https://arxiv.org/pdf/2003.14323.pdf)上面，由普林斯顿大学Alejandro等人合著，是属于Rethinking类的文章。题目的中文译名为：探索自监督预训练在视觉任务中的效果。

通读全文，作者的**主要目的**是调查哪些因素可以提升自监督预训练方法的效用。为了做到这一点，作者测试了各种自监督算法在**合成数据集**和下游任务(`downstream tasks`)中的表现。合成数据的好处，在于能够提供无限的注释图像，以及完全控制数据集的难度，方便对算法进行评估。

**论文的研究动机：**

我们观察到，许多现有的自监督评估常在`few-shot`设置或限制下游模型使用时进行，比如冻结预训练网络模型，只为下游任务训练一个线性层，但这并不能帮助我们获得更高的精度（性能对自动驾驶行人检测或者医学图像处理来说至关重要）。在不受标签数目限制的情况下，自我监督有多有用呢？这就是本文试图想要回答的问题。

**论文的主要发现：**

给定一个下游任务，随着标记数据的增多，模型的性能将会提高，并最终趋于稳定。但在实践中，标签预算是有限的，这个预算将决定train from scratch模型的准确性。在将微调(fine-tune)的自监督模型与从零开始训练(train from scratch)的基线进行比较时，可能会出现三种不同的结果(如图1所示)：a)自监督比基线更准确；b)自监督达到同样的准确度，使用的标注样本减少；c)自监督在相同的标注样本数量下达到相同的准确度。**在实验中，c)情况是最常出现的。**

<img src="https://raw.githubusercontent.com/fdujay/online_img/master/img/image-20200702133034001.png" alt="image-20200702133034001" style="zoom: 33%;" />

> 图1 自监督预训练方法在小标签预算中有用，但随着标签的增多，效用往往开始下降。

这说明，预培训的最大好处目前是在低数据体系下，在额外标签的任务性能达到稳定之前，效用(Utility)就接近于零。作者还发现，自监督在应用于更大的模型和更困难的数据时更有帮助。此外，不同pretext方法的性能在不同的downstream任务中不一致，并且线性评估性能与效用无关。

> 在有许多标记的例子的情况下，自监督仍然会有帮助，因为SGD训练不能保证达到全局最优，而自监督的预训练可以产生更好的表示，以帮助优化，更快更好的达到收敛（情况a）。

**论文的主要贡献：**

定义了数据利用效用公式：
$$
Utility = m/n-1
$$
其中，m是指从头开始训练达到性能A所需要的标签数目，n是指预训练微调达到性能A所需要的标签数目。

另外，作者构建了一个合成图像的基准(benchmark)。合成图像数据库提供了独特的优势：它可以轻易生成大量带标签的示例，还可以轻松探索各种下游任务，从分类到密集预测，从语义到几何任务。最后，它允许通过颜色、纹理和视点等因素精确控制数据的复杂性和下游任务的难度。本文的主要贡献是对所有这些方面进行了彻底的探索，从而为人们在何时何地可以期望自监督在实践中发挥作用提供了见解。

**启发和思考：**

对于自监督研究，一个很重要的问题就是，如何高效利用预训练中学习到的良好特征表示，保证细调模型在效用(Utility)降为零之前，就能达到稳定的downstream任务性能（避免出现c)情况）。否则，在达到一定精度之后，自监督并不能减少标注数据的依赖。

此外，自监督算法在一种情况下的性能不一定反映它在其他情况下的性能，这就强调了在不同情况下研究和评估预训练方法的重要性(Universal的方法是行不通的)。