---
layout:     post
title:      Rethinking Active Learning
subtitle:   主动学习在医学中的应用
date:       2020-06-26
author:     靳秋野
header-img: img/home-bg-art.jpg
catalog: true
tags:
    - Rethinking
---

参考博文：[周纵苇](http://www.zongweiz.com/)/[xinrui_zhuang](https://me.csdn.net/xinrui_zhuang)/[机器之心](https://mp.weixin.qq.com/s/qTZzQZEqHIJt_LAhYMd5lw)/[jiaotong_jin](https://blog.csdn.net/u013328485/article/details/96111113)
>
> 背景：目前推广应用的机器学习方法或模型主要解决分类问题，即给定一组数据（文本、图像、视频等），判断数据类别或将同类数据归类等，训练过程依赖于已标注类别的训练数据集。在实验条件下，这些方法或模型可以通过大规模的训练集获得较好的处理效果。然而在应用场景下，能够得到的数据实际上都没有进行人工标注处理，对这些数据进行类别标注所耗费的人力成本和时间成本非常巨大。在一些专门的应用领域，例如医学图像处理，只有专门学科的专业医生能够完成对医学影像图像的数据标注。显然，在这种情况下必须依赖大规模训练集才能使用的方法或模型都不再适用。

## 1. 简介

![img](https://raw.githubusercontent.com/fdujay/online_img/master/img/1689929-70c4a5d4d2cc8545.png)

为了减少对已标注数据的依赖，研究人员提出了主动学习（Active Learning）方法。主动学习通过某种策略找到未进行类别标注的样本数据中最有价值的数据，交由专家进行人工标注后，将标注数据及其类别标签纳入到训练集中迭代优化分类模型，改进模型的处理效果。

根据最有价值样本数据的获取方式区分，当前主动学习方法主要包括基于池的查询获取方法（query-acquiring/pool-based）和查询合成方法（query-synthesizing）两种。近年来提出的主动学习主要都是查询获取方法，即通过设计查询策略（抽样规则）来选择最具有价值信息的样本数据。与查询获取方法「选择（select）」样本的处理方式不同，查询合成方法「生成（generate）」样本。查询合成方法利用生成模型，例如生成式对抗网络（GAN, Generative Adversarial Networks）等，直接生成样本数据用于模型训练。

------

**问题背景：是不是训练数据集越多，深度学习的效果会越好呢？**

需要呈现的结果很简单，横坐标是训练集的样本数，纵坐标是分类的performance，如下图所示：

![img](https://raw.githubusercontent.com/fdujay/online_img/master/img/1689929-5e34911b5cdceaee.png)

> 图1-1 训练数据与性能的关系

如果答案是左图，那么就没什么可以说的了，去想办法弄到尽可能多的训练数据集就ok，但是现实结果是右图的红实线，一开始，训练集的样本数增加，分类器的性能快速地在上升，当训练集的样本数达到某一个临界值的时候，就基本不变了。

**也就是说，当达到了这个临界的数目时，再去标注数据的ground truth就是在浪费时间和金钱。**

> 这里需要说明的一点是，训练样本数的临界点大小和这个分类问题的难度有关，如果这个分类问题非常简单，如黑白图像分类（白色的是1，黑色的是0），那么这个临界值就特别小，往往几幅图就可以训练一个精度很高的分类器；如果分类问题很复杂，如判断一个肿瘤的良恶性（良性是0，恶性是1），那么这个临界值会很大，因为肿瘤的形状，大小，位置各异，分类器需要学习很多很多的样本，才能达到一个比较稳定的性能。

------

**解决思路：让数据饱和临界值变小，从而降低数据标注代价**

**本文用主动学习（Active Learning）的手段来挑选标注数据，从而找到一个更小的子集来达到最理想的性能。**主动学习（Active Learning），可以主动学习那些比较**“难的”**，**“信息量大的”**样本（hard mining）。关键点是每次都挑当前分类器分类效果不理想的那些样本（hard sample）给它训练，假设是训练这部分hard sample对于提升分类器效果最有效而快速。主动学习的核心问题，在于怎样**在不知道真正标签的情况下怎么去定义HARD sample？或者说怎么去描述当前分类器对于不同样本的分类结果的好坏？**之前的工作基本都是围绕这个问题展开的。

接下来，我们重点关注一下主动学习在医学图像处理中的应用。


## 2. 应用（持续更新）

### ***2.1 Fine-tuning Convolutional Neural Networks for Biomedical Image Analysis: Actively and Incrementally***

本篇论文发表于[CVPR2017](http://cvpr2017.thecvf.com/)，作者为美国亚利桑那州立大学着的在读博士生周纵苇。它主要解决的仍然是生物医学图像在用于深度学习时数据量过少的问题：如何使用尽可能少的标签数据来训练一个效果promising的分类器。作者提出了一个AIFT (active,incremental fine-tuning)网络，能够节约标注的时间和成本，把主动学习和迁移学习集成到一个框架。AIFT算法开始是直接使用一个预训练从未标注数据里找一些比较值得标注的样本，然后模型持续的加入新标注的数据，一直做微调。
AIFT方法是在CAD（计算机辅助诊断）系统的环境下使用，CAD可以生成候选集U，都是未标注数据，其中每一个候选样本（candidate）通过数据增强可以生成一系列的patches，由于这些patches来自于同一个候选样本，所以它们的标签跟该候选样本一致。

> 这与如今自监督学习中，对比监督方法类似，即来自相同样本的patches相似，不同样本的patches相异。

**定义：**由于深度学习的输出是属于某一类的概率（0～1），一个很直观的方法就是用“[熵（entropy）](https://en.wikipedia.org/wiki/Entropy)”来刻画信息量，把那些预测值模棱两可的样本挑出来，对于二分类问题，就是预测值越靠近0.5，它们的信息量越大。还有一个比较直观的方法是用“[多样性（diversity）](https://en.wikipedia.org/wiki/Diversity)”来刻画labeled data和unlabeled data的相似性。这两个方法都是在[“Active batch selection via convex relaxations with guaranteed solution bounds”](http://ieeexplore.ieee.org/abstract/document/7006697/)中被提出。是十分重要的两个Active Learning的选择指标。实验结果表明，与随机挑选相比，可以更快地达到临界拐点。

![img](https://raw.githubusercontent.com/fdujay/online_img/master/img/1689929-95bf7d0bd6942a4a.png)

> 图1-2 Active Learning的结构示意图。深度学习的优势在于，一开始你可以不需要有标记的数据集。

**优点：**

1. 从一个完全未标注的数据集开始，不需要初始的种子标注数据。

> 训练的初期不需要使用打好标签的数据对预训练的CNN模型进行训练，而是通过直接把未标注的数据导入预训练好的CNN网络中，得到预测值，挑出最难的，或者说是对于模型来说最不容易判断属于哪一类的图像来（文中采用的是熵和多样性的大小），人工打上标签再放进网络中进行训练。

2. 通过持续的fine-tuning而不是重复的重新训练来一步一步改善学习器。

> 一开始标注数据集L是空的，我们拿一个已经训练好了的CNN（比如AlexNet），让它在未标注数据集U中选b个候选集来找医生标注，这新标注的候选集将会放到标注数据集L中，来持续的增量式fine-tune那个CNN直到合格，通过实验发现，持续的fine-tuning CNN相比在原始的预训练中重复性的fine-tuning CNN，可以让数据集收敛更快。

3. 通过挖掘每一个候选样本的补丁的一致性来选择值得标注的候选集。

> 对每个候选样本，通过计算patch的熵和patch之间KL距离来衡量这个候选样本。如果熵越高，说明包含更多的信息，如果KL距离越大，说明patch间的不一致性大，所以这两个指标越高，越有可能对当前的CNN优化越大。对每个矩阵都可以生成一个包含patch的KL距离和熵的邻接矩阵R。

4. 自动处理噪音（少数服从多数）。

> 我们普遍都会使用一些自动的数据增强的方法，来提高CNN的表现，但是不可避免的给某些候选样本生成了一些难的样本，给数据集注入了一些噪音。所以为了显著的提高我们方法的鲁棒性，我们依照于当前CNN的预测，对每个候选样本只选择一部分的patch来计算熵和多样性。首先对每个候选样本的所有patch，计算平均的预测概率，如果平均概率大于0.5，我们只选择概率最高的部分patch，如果概率小于0.5，选最低的部分patch，再基于已经选择的patch，来构建得分矩阵R。

5. 只对每个候选集中小数量的补丁计算熵和KL距离，节约了计算，也大大节省了**计算损耗**。

**缺点：**

1. 既然用了迁移学习，那么一开始的CNN测试的效果肯定是一团糟，因为这个CNN是从自然图像中学过来的，没有学习过CT这种医学影像，所以这个loop的启动阶段，Active Learning的效果会没有random selecting好。不过很快，随着CNN慢慢地在labeled的CT上训练，Active Learning的效果会一下子超过random selecting。

2. 接下来讨论Continuous fine-tuning的细节，随着labeled data集变大，CNN需要一次次地被训练，有两种选择，一是每次都从ImageNetpretrained来的model来迁移，二是每次用当前的model上面迁移(ContinuousFine-tuning)。方法一的优点是模型的参数比较好控制，因为每次都是从头开始fine-tuning，但是缺点是随着labeled数据量大增加，GPU的消耗很大，相当于每次有新的标注数据来的时候，就把原来的model扔了不管，在实际应用中的代价还是很大的。第二种方法是从当前的model基础上做fine tune，在某种意义上knowledge是有记忆的，而且是连续渐进式的学习。问题在于参数不好控制，例如learning rate，需要适当的减小，而且比较容易在一开始掉入local minimum。
   